# Tarantool Raft

* **Status**: In progress
* **Start date**: 23-06-2019
* **Authors**: Vladislav Shpilevoy @Gerold103 \<v.shpilevoy@tarantool.org\>, Konstantin Osipov @kostja \<kostja@tarantool.org\>
* **Issues**: [#1146](https://github.com/tarantool/tarantool/issues/1146) [#3055](https://github.com/tarantool/tarantool/issues/3055) [#3234](https://github.com/tarantool/tarantool/issues/3234)

## Summary

Raft is a protocol to synchronously apply changes and make decisions on nodes of
a cluster.

The document describes design and implementation of Raft consensus protocol in
Tarantool.

At this moment it is not completed, still some open questions exist.

## Background and motivation

Raft is motivated by a long story about master promotion in Tarantool cluster.
Master promotion is a quite complex task, which consists of demotion of the
old master, synchronization of the nodes, and promotion of a new master. Despite
having so few steps, each of them is really complicated.

Master demotion should work even if the master is not available. Other nodes
should demote it by voting, and ignore all write-requests from it in case the
old master somehow returned.

Synchronization should be achieved in a major number of nodes, at least 50% + 1.

Master promotion should be done on one node only, obviously. Otherwise multiple
masters will appear.

If no new master was promoted in case of problems about syncing, or voting, then
it should be easy for a user to try the promotion again, or even do it
automatically.

There were a couple of standalone promotion implementations, but they have
failed because of 1) complexity, 2) bike reinvention. Promotion was split into
subtasks, each of which could be used even out of promotion.

First was SWIM - failure detection gossip protocol [#3234](https://github.com/tarantool/tarantool/issues/3234).
It is going to deal with failure detection in Raft, replace its original
heartbeats, or at least a part of them. It is one of killer features of
Tarantool Raft - heartbeats put a constant overhead regardless of number of
nodes, and even number of Rafts per one instance.

Second is the subject of the given RFC - Raft. The consensus protocol is going
to deal with uniqueness of promotion decision. It will ensure, that if multiple
instances want to be promoted, only one of them will succeed, and other nodes
will certainly know that. The leader will do promotions. That knowledge is
persisted.

Third and the last is promote procedure itself, which becomes in theory quite
trivial after Raft and SWIM are ready. But it is a subject for another RFC.

#### Out of promotion's scope

Raft is going to be used not only for promotion. Being quite a powerful
algorithm, its presence on board allows to replace some traditional consensus
tools such as Consul, etcd, Zookeeper, etc. To create a totally enclosed
ecosystem.

For example, Raft could be used to select several nodes of a cluster as Raft 
nodes, store on them a configuration of the cluster, and update it
synchronously. Other nodes will read that configuration and be sure, that it is
correct and every other node sees the same configuration.

## Detailed design

Raft in Tarantool is special. It is not a standalone tool, with its own
persistence, networking, and logging. It is a pure implementation of the
algorithm, based totally on the paper's math and virtual functions wherever
something is not related to the algorithm.

For the algorithm details the reader can address the original paper. Here and
below only API and surrounding tools are considered assuming that the reader is
familiar with the algorithm.

#### Persistence

The most basic thing Raft needs is persistence. Raft logs most of actions of
each node, some state details, and of course user requests, and it needs them
recovered on restart.

Raft has a small fixed size persistent state and a list of persisted logs.
State is characterized by current term and id of a candidate to which a vote was
given. List of logs is a list of records like {lsn, term, data}.

The list of logs should be compacted when it gets too big. In the Raft paper it
is called 'snapshot'. Since it strongly depends on a chosen storage, it should
be implemented out Raft, transparently to it.

Raft as an algorithm needs this API be implemented externally:

```C
/**
 * A virtual class to provide Raft with persistence. The log is
 * supposed to be able to rotate, be sequential, and do
 * snapshoting automatically.
 */
struct raft_log {
	int
	(*append_entry)(struct raft_log *log, const struct raft_log_entry *e);

	/**
	 * Deletion API. It happens, when a record is logged, but
	 * not committed and a new leader is elected, who doesn't
	 * even know about that record.
	 */
	void
	(*pop_entry)(struct raft_log *log);

	/** Load the record from the log with a given lsn. */
	const struct raft_log_entry *
	(*load_entry)(struct raft_log *log, int lsn);
};

/**
 * For who the instance has voted. It is used to avoid double vote
 * in case the instance has restarted.
 */
struct raft_log_vote {
	uuid_t candidate;
	/* ... */
};

/**
 * New term number. Since each log entry contains a term, this
 * record has no any specific data. Can appear solely when, for
 * example, the instance starts leader election, and increments
 * the term without applying any records.
 */
struct raft_log_term {
	/* ... */
};

/** User data to apply to the state machine. */
struct raft_log_data {
	data_t data;
	/* ... */
};

/**
 * It is supposed that user is likely to store all the logs in one
 * file. Records in such a file can be distinguished by these
 * identifiers, and Raft expects them when loads log.
 */
enum raft_log_entry_type {
	RAFT_LOG_RECORD_VOTE,
	RAFT_LOG_RECORD_TERM,
	RAFT_LOG_RECORD_ENTRY,
};

/** A log entry to persist/apply. */
struct raft_log_entry {
	enum raft_log_entry_type type;
	uint64_t lsn;
	uint64_t term;
	union {
		struct raft_log_vote vote;
		struct raft_log_term term;
		struct raft_log_entry entry;
	};
};
```

Snapshoting is the most dubious thing here. An initial idea was that it should
have been logically implemented in Raft, but physical implementation would be
provided by a user via another set of virtual functions. A new idea is that Raft
does not care about snapshoting, despite the original paper proposes the
opposite.

Raft only saves arbitrary data in a format 'void \*, size'. Format of that raw
data depends on a chosen storage, as well as snapshoting. For example this is
how a space replica-local storage could look:
`_raft_log = { cluster_id, lsn, term, any_data }`

Opened questions for the space storage:

1) What is `cluster_id`? It appears, because we want multiple Rafts per
   instance, and want to share one log storage for all of them. UUID here looks
   too heavy - it gives +17 bytes at least per each record. A string name as
   well can be too heavy. A possible workaround could be to store it as an
   integer + have a second system replica-local space for mapping
   `{name/uuid -> id}`. But still looks strange.

2) What is `any_data`? If it is just true 'any' type value, then how to snapshot
   these records? There should be a way how to compare two tuples
   `{lsn1, term1, data1}` and `{lsn2, term2, data2}`, and say if they are two
   versions of the same data. We need a comparator for this `any_data`. And it
   is not trivial. Just consider this example: `{promote = uuid1}` and
   `{promote = uuid2}`. Deep comparison of key-value pairs would say '!=', but
   in this concrete example only keys should be compared. At this moment the
   only way I see to identify these records is to store a key separately, as a
   scalar value: `_raft_log = { cluster_id, lsn, term, scalar_key, any_data }`.
   Snapshot will identify old records by `{lsn, term, scalar_key}` keys.

These questions should be discussed and decisions on what to do should be made
before we start to implement anything.

#### Transport

Raft requires the following API be implemented externally:

```C
struct raft_node_io {
	/* Socket, address, ... */
};

struct raft_request_append_entries {
	uint64_t term;
	uint32_t log_count;
	struct raft_log_entry *logs;
	/* ... */
};

struct raft_request_vote {
	uint64_t term;
	uuid_t candidate;
	/* ... */
};

int
raft_send_append_entries(struct raft_node_io *io,
			 const struct raft_request_append_entries *req);

int
raft_send_vote(struct raft_node_io *io, const struct raft_request_vote *req);
```

#### Failure detection

Since Raft doesn't have own sockets, it can't detect failures by itself. Raft
uses SWIM for that. A SWIM instance is created outside of Raft, connects to
other Raft nodes, and the SWIM cluster works in a normal mode. Raft subscribes
on SWIM cluster changes. When a node fails, and it is a leader, Raft starts
doing something. For example, tries to choose a new leader.

Possible API to be called by SWIM is quite simple:
```C
void
raft_on_member_update(struct trigger *t, void *event)
{
	/*
	 * Extract member from @a event, and Raft from @a t. If
	 * the member belongs to Raft and is a leader, then start
	 * a new leader election.
	 */
}
```

#### Behaviour when < 3 nodes

Looks like when there are only 2 nodes available, Raft can't make decisions. But
in fact it can. Unfortunately, for that both nodes should be alive, because
in such a small cluster majority of modes = the whole cluster.

#### Forced election

Sometimes it happens, that something is wrong with the cluster. It is broken due
to an inaccurate configuration, or due to a bug inside an application or
Tarantool. In such a case it is essential to provide a 'root' API for a system
administrator so as he could repair the cluster swiftly.

Since Raft by design does not describe any 'forced' actions, it should be
created solely by Tarantool. It is proposed to introduce a new log record
called, ironic, `promote`. It contains identifier of a leader, a new term,
allowed to be set manually, and when an instance receives such a record with a
term > current one, it immediately believes into the new leader.

```C
/** Log record. */
struct raft_log_promote {
	uint64_t term;
	uuid_t leader;
	/* ... */
};

struct raft_request_promote {
	uint64_t term;
	uuid_t leader;
	/* ... */
};

int
raft_log_append_promote(struct raft_log *log, const struct raft_log_promote *e);

int
raft_send_log_promote(struct raft_node_io *io,
		      const struct raft_request_promote *e);

/** Public API for administrators. */
int
raft_promote(uint64_t term);
```

#### Deletion

Raft does not describe how to delete keys permanently. So as they even do not
exist in a next snapshot after deletion. There is a proposal to introduce an
attribute for `struct raft_log_entry` as
`enum {RAFT_LOG_REPLACE, RAFT_LOG_DELETE}`. During snapshot if the latest record
for a key is `RAFT_LOG_DELETE`, then it is totally dropped.

#### Interface

Besides inevitable functions like `raft_new`/`delete`/`add_node`/`remove_node`/
etc, there will be a couple of core logic functions. They made Raft API looking
like WAL.

```C
/** Synchronously write user defined data to the Raft group. */
int
raft_write(struct raft *raft, const char *data, size_t data_size);

/**
 * Subscribe on an event when a new record is applied to the state
 * machine.
 */
void
raft_on_commit(struct raft *raft, struct trigger *t);
```

## Rationale and alternatives

#### Persistence

Besides virtual functions implemented externally there were 2 alternatives:

- Use replica local spaces. Replica locality would help to do not mess logs of
  different Raft nodes via built-in replication. Mess is a lack of global source
  of unique sequence numbers, and logs reordering, what was a big pain in the
  ass in the earlier promotion implementations. Raft would have its own
  replication. Replica local spaces would solve the problem of persistence,
  snapshoting, recovery, old xlog files GC. But cons is that Raft wouldn't be
  able to work before box.cfg. Or at least wouldn't be able to vote nor become a
  leader. Only read records into a volatile log list. Moreover, Raft needs fsync
  for each request. In Tarantool there is no way how to fsync individual
  transactions for certain spaces.

- Store logs in a separate xlog file, not a part of WAL. Or even not xlog, but
  with its own structure, very simplified. Pros: Raft becomes totally
  independent from box, can be started before box.cfg. Cons: need to implement
  snapshoting, GC, and how to append to these files - TX thread should not write
  to disk. Probably, coio would be enough for IO.

#### Replication

In the final solution Raft uses virtual transport function of kind
`send_smth()`. There was an alternative with Raft's internal TCP sockets. Pros:
simple, keeps Raft independent. Cons: probably we might want to piggyback Raft
messages via replication sockets, or base Raft on UDP. Dedicated static socket
implementation would not allow this.

#### Existing implementations
