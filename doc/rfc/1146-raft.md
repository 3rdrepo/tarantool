# Tarantool Raft

* **Status**: In progress
* **Start date**: 23-06-2019
* **Authors**: Vladislav Shpilevoy @Gerold103 \<v.shpilevoy@tarantool.org\>, Konstantin Osipov @kostja \<kostja@tarantool.org\>
* **Issues**: [#1146](https://github.com/tarantool/tarantool/issues/1146) [#3055](https://github.com/tarantool/tarantool/issues/3055) [#3234](https://github.com/tarantool/tarantool/issues/3234)

## Summary

Raft is a protocol to synchronously apply changes and make decisions on nodes of
a cluster.

The document describes design and implementation of Raft consensus protocol in
Tarantool.

At this moment it consists of points to discuss; possible interfaces; open
questions; tasks the Raft is intended to solve in Tarantool.

## Background and motivation

Raft is motivated by a long story about master promotion in Tarantool cluster.
Master promotion is a quite complex task, which consists of demotion of the
old master, synchronization of the nodes, and promotion of a new master. Despite
having so few steps, each of them is really complicated.

Master demotion should work even if the master is not available. Other nodes
should demote it by voting, and ignore all write-requests from it in case the
old master somehow returned.

Synchronization should be achieved in a major number of nodes, at least 50% + 1.

Master promotion should be done on one node only, obviously. Otherwise multiple
masters will appear.

If no new master was promoted in case of problems about syncing, or voting, then
it should be easy for a user to try the promotion again, or even do it
automatically.

There were a couple of standalone promotion implementations, but they have
failed because of 1) complexity, 2) bike reinvention. Promotion was split into
subtasks, each of which could be used even out of promotion.

First was SWIM - failure detection gossip protocol [#3234](https://github.com/tarantool/tarantool/issues/3234).
It is going to deal with failure detection in Raft, replace its original
heartbeats, or at least a part of them. At least it was thought like that.

Second is the subject of the given RFC - Raft. The consensus protocol is going
to deal with uniqueness of promotion decision. It will ensure, that if multiple
instances want to be promoted, only one of them will succeed, and other nodes
will certainly know that. That knowledge is persisted.

Third and the last is promote procedure itself, which becomes in theory quite
trivial after Raft and SWIM are ready. But it is a subject for another RFC.

#### Customers

Raft is going to be used not only for promotion. There are people who is going
to use it as is for their own purposes. These people are Konstantin Nazarov, and
Vladimir Perepelitza, as it is known. Their tasks are quite similar - they both
want Tarantool be able to work as Consul/etcd/Zookeeper. To create a totally
enclosed ecosystem.

The task is to select several nodes of a cluster as Raft nodes, store on them a
configuration of the cluster, and update it synchronously. Other nodes will
read that configuration and be sure, that it is correct and every other node
sees the same configuration.

It can be said for sure, that there are more people who want the same, so it
should be taken into account when making decisions affecting the public API, and
behaviour.

## Detailed design

This section is rather some possible ways of to where the detailed design should
evolve from one of several starting points.

#### Persistence

The most basic thing Raft needs is persistence. Raft logs most of actions of
each node, some state details, and of course user requests, and it needs them
recovered on restart.

User requests are likely to be simple key-value pairs, but it is a subject of
different section on public API, below.

There are 3 ways how to persist:

- Provide a virtual functions API to allow a user to set functions `store_log`,
  `recover_log` etc. A user either uses one of the next ways, or invents its
  own, it does not matter. Pros: Raft becomes simpler. Cons: it is not automatic
  enough, user still needs to do something, and it is not an optimal solution in
  terms of performance.

- Use replica local spaces. Replica locality helps us to do not mess logs of
  different Raft nodes via built-in replication. Mess is a lack of global source
  of unique sequence numbers, and logs reordering, what was a big pain in the
  ass in the earlier promotion implementations. Raft has its own replication.
  Replica local spaces solves the problem of persistence, snapshoting, recovery,
  old xlog files GC. But cons is that Raft won't be able to work before box.cfg.
  Or at least won't be able to vote nor become a leader. Only read records in a
  volatile log list. Moreover, Raft needs fsync for each request. In Tarantool
  there is no way how to fsync individual transactions for certain spaces.

- Store logs in a separate xlog file, not a part of WAL. Or even not xlog, but
  with its own structure, very simplified. Pros: Raft becomes totally
  independent from box, can be started before box.cfg. Cons: need to implement
  snapshoting, GC, and how to append to these files - TX thread should not write
  to disk. Probably, coio would be enough for IO.

Personally, I vote for the last option - it is the most easy to extend and
the most independent solution. Talking of customers, as I know, Kostja N. needs
it, because he wants to start Raft right on the storage nodes, which leads him
to necessity to retrieve configuration before box.cfg. But configuration
retrieval requites Raft. If Raft depends on box.cfg, it would be a cyclic
dependency.

#### Multiple Rafts

Only two options: ether we follow SWIM's strategy and allow multiple Raft nodes
per one instance, or not. Allowance strongly simplifies testing, as usual. I
vote for multiple instances.

#### Transport

How should Raft replicate logs, in case we are using one of the solutions
from "Persistence" section above?

- Raft implementation has its own TCP sockets. Pros: simple, keeps Raft
  independent. Cons: probably we might want to piggyback Raft messages via
  replication sockets? Dedicated sockets would not allow this.

- Raft has virtual functions like `send_log`, `recv_log`, which are called by
  box internals, probably by replication relay/applier. Pros: can piggyback
  Raft messages via existing connections. Cons: complexity, works in fullmesh
  only, and not sure how can work before box.cfg.replication is set. Probably
  virtual transport would require to implement both types of transport, and
  would use real sockets implementation before box.cfg.replication is set.

#### Failure detection

Raft uses failure detection to check if a leader is alive. If it is not, a new
election is started. SWIM could help with failure detection, but is it enough?
If Raft will have its own TCP connections, it would be strange not to use them
and rely on UDP SWIM only. Even if Raft would use virtual transport functions,
they anyway are going to use TCP underneath.

In other words, should Tarantool's implementation of Raft care about failure
detection?

In case it does not have own detection, I can't imagine how to use it in user
applications. Also without own failure detection it should obtain information
about failures from outside. Probably some hook callbacks called by internal
Tarantool services like replication.

#### Cluster changes

How to update Raft configuration? How to add/remove members? The original Raft
describes 2 phases of configuration update. It ensures, that during each phase
it is impossible to select more than one leader, and at the same time user
requests are being serviced in a normal mode. Do we have anything against the
original Raft here?

#### Behaviour when < 3 nodes

Raft says, that it needs a majority of nodes to make decisions, so in the
vanilla version < 3 nodes are not allowed. Are we going to allow it anyhow?
See the next section, as it is related.

#### Forced election

Raft does not say anything about forced leader election. Moreover, according to
it followers will ignore any attempts to elect a new leader, until they see the
current leader alive.

Do we need to break that rule, and have an ability to change a leader
forcefully?

#### Proxy to leader

In the Raft paper there is no pure proxy from followers to the leader. If a
client interacts with a follower, it gets an error saying an address of the
current leader. Do we need a transparent proxy here?

#### Identification of nodes

According to the original Raft, nodes are identified by unique numbers, and
there we face with exactly the same problem as about SWIM - unique numbers and
IP/port addresses does not work. I propose to use UUIDs again. Is it ok?

#### Interface

Besides inevitable functions like `raft_new`/`delete`/`add_node`/`remove_node`/
etc, there will be a few core logic functions. They should be generic enough to
be easy to use for any synchronous requests, decisions, reads. A possible
variant is below. Looks like a synchronous simplified space, probably available
before box.cfg.

```C
/**
 * Synchronously publish @a value under @a key. If there are
 * multiple attempts at the same time from other servers to post
 * the same key, then only one of them will succeed. Others will
 * see, that a leader has uncommitted change for that key, and
 * will get an error. In case that key already exists, but is
 * already committed, it is rewritten.
 */
int
raft_replace_key(struct raft *raft, const char *key, const char *value,
		 size_t value_size);

/**
 * The same as replace, but returns an error, if such a key
 * already exists, regardless whether it is committed or not.
 */
int
raft_insert_key(struct raft *raft, const char *key, const char *value,
		size_t value_size);

/** Synchronously delete a key from all the nodes. */
int
raft_delete_key(struct raft *raft, const char *key);

/** Read the latest value of a key from the leader. */
int
raft_read_key(struct raft *raft, const char *key, const char **value,
	      size_t *value_size);

/** Subscribe on commits of all keys. */
void
raft_on_commit(struct raft *raft, struct trigger *t);
```

An example of usage for promotion:
```C
/** Called on a node to promote. */
int
box_ctl_promote(void)
{
	return raft_replace(internal_raft, "master", my_uuid, 16);
}

/** Called on a leader. */
void
raft_on_commit(struct raft *raft, struct trigger *t)
{
	if (key == "master") {
		/*
		 * Do promotion. All updates are blocked while
		 * the triggers work.
		 * ...
		 */
	}
}
```

For cfg update is exactly the same, but key is `config`, and value is the
configuration blob. On the leader Kostja doesn't need even to react anyhow on
that update.

## Rationale and alternatives

At this moment the whole design is just a set of alternatives.
